{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89b43adc-cb05-4ca3-80c5-e35e5e65e04f",
   "metadata": {},
   "source": [
    "# SQuAD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84acf928-5e72-4d54-9298-a788350de61f",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786c4dda-ee84-4ebb-bc62-55e0a9ce8c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "squad_path = 'datasets/squad.json'\n",
    "\n",
    "# Load the JSON file\n",
    "with open(squad_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "    # data = data.drop(columns=['version'])\n",
    "\n",
    "rows = []\n",
    "for entry in data[\"data\"]:\n",
    "    title = entry[\"title\"]\n",
    "    for paragraph in entry[\"paragraphs\"]:\n",
    "        rows.append({\"title\":title, \"data\":paragraph[\"context\"]})    \n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8f2bab-2386-48cf-87e2-75ad2173b64b",
   "metadata": {},
   "source": [
    "## Segement Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0157afa7-2397-495a-8382-dd3ac83b715b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9925d04-8266-455f-acd2-a8c822552184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ce964c-d44e-4eac-80da-5aeeef4ad492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load ROCStories dataset.\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df\n",
    "\n",
    "# clean and normalize text\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and normalize text.\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove excessive whitespace\n",
    "    text = text.strip()  # Remove leading/trailing whitespace\n",
    "    text = re.sub(r'[^\\w\\s\\.\\,\\']', '', text)  # Remove special characters\n",
    "    return text\n",
    "\n",
    "# tokenization and processing\n",
    "def process(df):\n",
    "    \"\"\"Process stories into a tokenized and cleaned format.\"\"\"\n",
    "    processed = []\n",
    "    for _, row in df.iterrows():\n",
    "        # story_id = row['storyid']\n",
    "        # story_title = clean_text(row['storytitle'])\n",
    "        title = row['title']\n",
    "        text = row['data']\n",
    "        sentences = [clean_text(sentence) for sentence in text.split('.')]\n",
    "        processed.append({'Title': title, 'Sentences': sentences})\n",
    "    return processed\n",
    "\n",
    "# generate embeddings\n",
    "def generate_embeddings(processed, model_name='all-MiniLM-L6-v2'):\n",
    "    \"\"\"Generate sentence embeddings using Sentence Transformers.\"\"\"\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = []\n",
    "    metadata = []\n",
    "\n",
    "    for entry in processed:\n",
    "        title = entry['Title']\n",
    "        sentences = entry['Sentences']\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            embedding = model.encode(sentence, convert_to_tensor=True).cpu().numpy()\n",
    "            embeddings.append(embedding)\n",
    "            metadata.append({\n",
    "                'Title': title,\n",
    "                'SentenceIndex': i,\n",
    "                'Sentence': sentence\n",
    "            })\n",
    "\n",
    "    embeddings = np.array(embeddings)\n",
    "    return embeddings, metadata\n",
    "\n",
    "# store embeddings in FAISS\n",
    "def build_faiss_index(embeddings):\n",
    "    \"\"\"Build and store FAISS index for retrieval.\"\"\"\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "# save preprocessed data\n",
    "def save_preprocessed_data(metadata, index, index_file='faiss_index'):\n",
    "    \"\"\"Save metadata and FAISS index.\"\"\"\n",
    "    # save metadata\n",
    "    pd.DataFrame(metadata).to_csv('metadata.csv', index=False)\n",
    "    # save FAISS index\n",
    "    faiss.write_index(index, index_file)\n",
    "\n",
    "# main preprocessing pipeline\n",
    "def preprocess_pipeline(df):\n",
    "    \"\"\"Complete preprocessing pipeline for ROCStories dataset.\"\"\"\n",
    "    print(\"Loading data...\")\n",
    "    # df = load_data(file_path)\n",
    "\n",
    "    print(\"Processing and cleaning stories...\")\n",
    "    processed = process(df)\n",
    "\n",
    "    print(\"Generating embeddings...\")\n",
    "    embeddings, metadata = generate_embeddings(processed)\n",
    "\n",
    "    print(\"Building FAISS index...\")\n",
    "    index = build_faiss_index(embeddings)\n",
    "\n",
    "    print(\"Saving preprocessed data...\")\n",
    "    save_preprocessed_data(metadata, index, index_file='faiss_index')  # pass FAISS index\n",
    "    print(\"Preprocessing complete!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # file_path = 'datasets\\ROCStories_winter2017.csv'\n",
    "    preprocess_pipeline(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47aaa6c8-fade-46d0-8028-73c1c4a155b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = pd.read_csv('metadata.csv')\n",
    "processed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7e4186-8f32-4912-804c-2dc634d1bf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.save('embeddings.npy', index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b65c5a-2b0c-4666-8c75-525715d5f936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the FAISS index\n",
    "index = faiss.read_index('faiss_index')\n",
    "\n",
    "# load metadata\n",
    "metadata = pd.read_csv('metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017a4023-a454-448d-9ed0-415a5c26d40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# load embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# example query\n",
    "query = \"It's so good to be alive\"\n",
    "\n",
    "# generate query embedding\n",
    "query_embedding = model.encode(query, convert_to_tensor=True).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4eded9-b495-4aea-a16d-b2df297295cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of closest matches to retrieve\n",
    "top_k = 10\n",
    "\n",
    "# search\n",
    "distances, indices = index.search(query_embedding.reshape(1, -1), top_k)\n",
    "\n",
    "for i, idx in enumerate(indices[0]):\n",
    "    print(f\"Result {i + 1}:\")\n",
    "    print(f\"Sentence: {metadata.iloc[idx]['Sentence']}\")\n",
    "    print(f\"Story Title: {metadata.iloc[idx]['StoryTitle']}\")\n",
    "    print(f\"Distance: {distances[0][i]}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
